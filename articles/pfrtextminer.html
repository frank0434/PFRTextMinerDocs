<!DOCTYPE html>
<!-- Generated by pkgdown: do not edit by hand --><html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
<meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1.0">
<title>PFRTextMiner: Webscraping and Text Mining at Plant &amp; Food Research • PFRTextMiner</title>
<!-- jquery --><script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.4.1/jquery.min.js" integrity="sha256-CSXorXvZcTkaix6Yvo6HppcZGetbYMGWSFlBw8HfCJo=" crossorigin="anonymous"></script><!-- Bootstrap --><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/twitter-bootstrap/3.4.1/css/bootstrap.min.css" integrity="sha256-bZLfwXAP04zRMK2BjiO8iu9pf4FbLqX6zitd+tIvLhE=" crossorigin="anonymous">
<script src="https://cdnjs.cloudflare.com/ajax/libs/twitter-bootstrap/3.4.1/js/bootstrap.min.js" integrity="sha256-nuL8/2cJ5NDSSwnKD8VqreErSWHtnEP9E7AySL+1ev4=" crossorigin="anonymous"></script><!-- bootstrap-toc --><link rel="stylesheet" href="../bootstrap-toc.css">
<script src="../bootstrap-toc.js"></script><!-- Font Awesome icons --><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.12.1/css/all.min.css" integrity="sha256-mmgLkCYLUQbXn0B1SRqzHar6dCnv9oZFPEC1g1cwlkk=" crossorigin="anonymous">
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.12.1/css/v4-shims.min.css" integrity="sha256-wZjR52fzng1pJHwx4aV2AO3yyTOXrcDW7jBpJtTwVxw=" crossorigin="anonymous">
<!-- clipboard.js --><script src="https://cdnjs.cloudflare.com/ajax/libs/clipboard.js/2.0.6/clipboard.min.js" integrity="sha256-inc5kl9MA1hkeYUt+EC3BhlIgyp/2jDIyBLS6k3UxPI=" crossorigin="anonymous"></script><!-- headroom.js --><script src="https://cdnjs.cloudflare.com/ajax/libs/headroom/0.11.0/headroom.min.js" integrity="sha256-AsUX4SJE1+yuDu5+mAVzJbuYNPHj/WroHuZ8Ir/CkE0=" crossorigin="anonymous"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/headroom/0.11.0/jQuery.headroom.min.js" integrity="sha256-ZX/yNShbjqsohH1k95liqY9Gd8uOiE1S4vZc+9KQ1K4=" crossorigin="anonymous"></script><!-- pkgdown --><link href="../pkgdown.css" rel="stylesheet">
<script src="../pkgdown.js"></script><meta property="og:title" content="PFRTextMiner: Webscraping and Text Mining at Plant &amp; Food Research">
<meta property="og:description" content="PFRTextMiner">
<!-- mathjax --><script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js" integrity="sha256-nvJJv9wWKEm88qvoQl9ekL2J+k/RWIsaSScxxlsrv8k=" crossorigin="anonymous"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/config/TeX-AMS-MML_HTMLorMML.js" integrity="sha256-84DKXVJXs0/F8OTMzX4UR909+jtl4G7SPypPavF+GfA=" crossorigin="anonymous"></script><!--[if lt IE 9]>
<script src="https://oss.maxcdn.com/html5shiv/3.7.3/html5shiv.min.js"></script>
<script src="https://oss.maxcdn.com/respond/1.4.2/respond.min.js"></script>
<![endif]-->
</head>
<body data-spy="scroll" data-target="#toc">
    <div class="container template-article">
      <header><div class="navbar navbar-default navbar-fixed-top" role="navigation">
  <div class="container">
    <div class="navbar-header">
      <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target="#navbar" aria-expanded="false">
        <span class="sr-only">Toggle navigation</span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
      </button>
      <span class="navbar-brand">
        <a class="navbar-link" href="../index.html">PFRTextMiner</a>
        <span class="version label label-default" data-toggle="tooltip" data-placement="bottom" title="Released version">0.2.0</span>
      </span>
    </div>

    <div id="navbar" class="navbar-collapse collapse">
      <ul class="nav navbar-nav">
<li>
  <a href="../index.html">
    <span class="fas fa fas fa-home fa-lg"></span>
     
  </a>
</li>
<li>
  <a href="../reference/index.html">Reference</a>
</li>
<li class="dropdown">
  <a href="#" class="dropdown-toggle" data-toggle="dropdown" role="button" aria-expanded="false">
    Articles
     
    <span class="caret"></span>
  </a>
  <ul class="dropdown-menu" role="menu">
<li>
      <a href="../articles/pfrtextminer.html">PFRTextMiner: Webscraping and Text Mining at Plant &amp; Food Research</a>
    </li>
  </ul>
</li>
      </ul>
<ul class="nav navbar-nav navbar-right">
<li>
  <a href="https://github.com/PlantandFoodResearch/PFRTextMiner/">
    <span class="fab fa fab fa-github fa-lg"></span>
     
  </a>
</li>
      </ul>
</div>
<!--/.nav-collapse -->
  </div>
<!--/.container -->
</div>
<!--/.navbar -->

      

      </header><div class="row">
  <div class="col-md-9 contents">
    <div class="page-header toc-ignore">
      <h1 data-toc-skip>PFRTextMiner: Webscraping and Text Mining at Plant &amp; Food Research</h1>
                        <h4 class="author">Blake List</h4>
            
            <h4 class="date">2020-05-06</h4>
      
      <small class="dont-index">Source: <a href="https://github.com/PlantandFoodResearch/PFRTextMiner/blob/master/vignettes/pfrtextminer.Rmd"><code>vignettes/pfrtextminer.Rmd</code></a></small>
      <div class="hidden name"><code>pfrtextminer.Rmd</code></div>

    </div>

    
    
<div id="introduction" class="section level1">
<h1 class="hasAnchor">
<a href="#introduction" class="anchor"></a>Introduction</h1>
<p>PFRTextMiner is, like its name suggests, an R package designed for webscraping and text mining. This vignette will take you step by step through the process of how to use the package, with a couple of use cases in mind. To the uninitiated, ‘scraping’ or collecting data from the web can be a daunting task. Plant &amp; Food Research’s Microsoft Sharepoint site, iPlant, does not make this any easier. The goal here is to automate the tedious tasks, while educating the user throughout the process. When it comes to text mining, topics like ‘Natural Language Processing’ spring to mind. It is one thing to have text data that you wish to analyse, and a whole other when dealing with tidy text data. That said, this vignette will try to explain the meaning of ‘tidy’ text data, as well as some simple methods in extracting information from it.</p>
<div id="what-is-webscraping-and-text-mining" class="section level2">
<h2 class="hasAnchor">
<a href="#what-is-webscraping-and-text-mining" class="anchor"></a>What is webscraping and text mining?</h2>
<p>In a nutshell, web scraping is the process of extracting meaningful data from websites. It is implemented by a function which sends a “GET” query to a specific website. Then, it parses an HTML document based on the received result. One can view the HTML source code of a website usually by pushing the F12 key. After it’s done, the scraping function searches for the data you need within the document, and, finally, converts it into the specified format. One of the main challenges with webscraping is that most websites use a different layout. This can make it difficult to extract deeply embedded elements of a webpage, such as the rows or columns in a table on a website. Another challenge is dealing with authentication. One of the most popular uses of webscraping is to extract data from social media such as Twitter or Facebook. These sites usually have an API (a certain procedure to follow in order to communicate with the website) that makes webscraping a little more structured. iPlant, however, does not (at this stage) have an API compatible with the R language. Thankfully, the PFRTextMiner package provides a way to manoeuvre this.</p>
<p>Text mining, on the other hand, works with textual data (e.g. scientific reports, spray diaries, sensory comments) that are often in an untidy format. This means the data is not in a format ready for textual analysis. It could be that the text contains unnecessary references, headers and footers, or perhaps the text is all grouped together in one big character vector. A tidy text format for, say, a scientific report may be, one sentence per line of a dataframe, where headers and titles have been removed or are used to separate bodies of text. This then allows for simple natural language processing techniques to be applied which could extract the sentiment of the text - how positive or negative the comment is, summarise the data into important sentences, extract keywords based on term frequency (how many times a word appears in a text), and much more.</p>
</div>
<div id="what-can-i-do-with-pfrtextminer" class="section level2">
<h2 class="hasAnchor">
<a href="#what-can-i-do-with-pfrtextminer" class="anchor"></a>What can I do with PFRTextMiner?</h2>
<p>PFRTextMiner currently contains seven functions that allow data to be extracted from iPlant, and text data to be processed and analysed. We will dive more in depth into the functionality of the functions but for now here is a list of things that can be done with the package.</p>
<ul>
<li>Authenticate and extract a table or list from an iPlant page as a dataframe in R.</li>
<li>Specify a page containing multiple files of any file type and download them to a local folder.</li>
<li>Automatically extract all text from one or multiple documents (.pdf, .doc, .docx) and convert this to a dataframe of tidy text containing each document.</li>
<li>Explicitly specify the body of text and extract this from one or multiple documents.</li>
<li>Summarise this text into the n most important sentences.</li>
<li>Extract the most important and frequently occurring keywords from the text.</li>
<li>Analyse the sentiment of the text as being positive, negative or neutral.</li>
</ul>
<p>Like almost all packages, development of PRFTextMiner is ongoing, and driven by user feedback. So if there is something you need done that PFRTextMiner does not already do, please get in contact with the author!</p>
</div>
</div>
<div id="how-to-use-pfrtextminer" class="section level1">
<h1 class="hasAnchor">
<a href="#how-to-use-pfrtextminer" class="anchor"></a>How to use PFRTextMiner</h1>
<div id="scraping-tables-and-lists-from-iplant" class="section level2">
<h2 class="hasAnchor">
<a href="#scraping-tables-and-lists-from-iplant" class="anchor"></a>Scraping tables and lists from iPlant</h2>
<p>We will first imagine there is some important information on an iPlant page contained in a table or list. Basically, we want exactly what we see online, but as a tidy and convenient dataframe in R. We want the column headers to be the same and we also do not want to automatically fill in the missing values, or have the data slide over or wrap around. We also have to deal with signing in to the iPlant page like we would in a web browser. Sounds difficult? I assure you it’s not. Let’s begin.</p>
<p>For the purpose of this demonstration, we will set a test username and password. When using the package functions, if authentication is needed, the username will be taken from the system environment and the user will be prompted for their password.</p>
<div class="sourceCode" id="cb1"><html><body><pre class="r"><span class="fu"><a href="https://rdrr.io/r/base/library.html">library</a></span>(<span class="no">PFRTextMiner</span>)

<span class="co"># Test credentials.</span>
<span class="no">test_user</span> <span class="kw">&lt;-</span> <span class="st">"SPTR1"</span>
<span class="no">test_pass</span> <span class="kw">&lt;-</span> <span class="st">"$ptr1001"</span>
<span class="fu"><a href="https://rdrr.io/r/base/Sys.setenv.html">Sys.setenv</a></span>(<span class="st">"USER"</span> <span class="kw">=</span> <span class="no">test_user</span>)
<span class="fu"><a href="https://rdrr.io/r/base/Sys.setenv.html">Sys.setenv</a></span>(<span class="st">"PASSWORD"</span> <span class="kw">=</span> <span class="no">test_pass</span>)

<span class="co"># The url we want to get the table from.</span>
<span class="no">test_url</span> <span class="kw">&lt;-</span> <span class="st">"https://iplant.plantandfood.co.nz/project/datamgmt/Lists/Data%20Improvement%20Plan/AllItems.aspx"</span>

<span class="co"># Use the function scrape_list to extract the table from iPlant as a dataframe.</span>
<span class="no">df</span> <span class="kw">&lt;-</span> <span class="fu"><a href="../reference/scrape_list.html">scrape_list</a></span>(<span class="kw">url</span> <span class="kw">=</span> <span class="no">test_url</span>)
<span class="kw pkg">knitr</span><span class="kw ns">::</span><span class="fu"><a href="https://rdrr.io/pkg/knitr/man/kable.html">kable</a></span>(<span class="fu"><a href="https://rdrr.io/r/utils/head.html">head</a></span>(<span class="no">df</span>[<span class="fl">1</span>:<span class="fl">5</span>]))</pre></body></html></div>
<table class="table">
<thead><tr class="header">
<th align="left">Title</th>
<th align="left">Data_improvement_item</th>
<th align="left">Assigned_to</th>
<th align="left">Start_date</th>
<th align="left">Due_date</th>
</tr></thead>
<tbody>
<tr class="odd">
<td align="left">Improve iPlant team page</td>
<td align="left">Work with Sheree to update team iPlant page so document library suits our needs.</td>
<td align="left">Melanie Burns</td>
<td align="left">2018-06-11 00:00:00</td>
<td align="left">2018-08-28 00:00:00</td>
</tr>
<tr class="even">
<td align="left">Standardise excel template</td>
<td align="left">Customise PFR excel template to suit team need. Ensure all team members are using it.</td>
<td align="left">Carmel Woods</td>
<td align="left">2018-07-31 00:00:00</td>
<td align="left">2018-08-31 00:00:00</td>
</tr>
<tr class="odd">
<td align="left">File names</td>
<td align="left">Agree with team on format of naming files.</td>
<td align="left">Melanie Burns</td>
<td align="left">2018-06-03 00:00:00</td>
<td align="left">2018-07-31 00:00:00</td>
</tr>
<tr class="even">
<td align="left">So that was very short. What</td>
<td align="left">is <code>scrape_list</code> actually doing? Looking at the source code, it first gets the user’s cr</td>
<td align="left">edentials, eithe</td>
<td align="left">r from the system envi</td>
<td align="left">ronment or by prompt, and retrieves the HTML data for that page. This HTML response is then parsed to text and the table or lists universally unique identifier (UUID) is extracted using a call to the function <code>scrape_uuid</code>. This then returns a modified format to the original url that contains the uuid. Another request to the page is made, this time with the formatted url and the HTML data is extracted. The function then uses the <code>XML</code> package to extract each column name and row element and recursively add this information to a dataframe to be returned as the final output.</td>
</tr>
</tbody>
</table>
<p>As a side note, due to the format of the HTML data on iPlant, more columns will be scraped than what is visible on the page. One can use the <code>select</code> function or <code>[1:n]</code> subset to return the desired columns.</p>
</div>
<div id="reading-excel-files-from-iplant-into-r" class="section level2">
<h2 class="hasAnchor">
<a href="#reading-excel-files-from-iplant-into-r" class="anchor"></a>Reading Excel files from iPlant into R</h2>
<p>One of the issues with reading Excel spreadsheets directly from iPlant is that the url format that Microsoft SharePoint uses to view the spreadsheets in a web browser is not compatible with any function in R that makes response requests to webpages, like GET from httr. This means that we have to first format the url before reading it in.<code>scrape_xl</code> provides a way to automatically format the url and reads it directly into a dataframe.</p>
<div class="sourceCode" id="cb2"><html><body><pre class="r"><span class="co"># The url of the Excel spreadsheet we want to read in.</span>
<span class="no">url</span> <span class="kw">&lt;-</span> <span class="st">"https://iplant.plantandfood.co.nz/project/I180822/_layouts/15/WopiFrame2.aspx?sourcedoc=/project/I180822/Research/T3_Maj_Diameter_Manual.xlsx&amp;action=default"</span>

<span class="co"># Use the function scrape_xl to automatically format the url and read the Excel data from iPlant into R as a dataframe.</span>
<span class="no">xl_df</span> <span class="kw">&lt;-</span> <span class="fu"><a href="../reference/scrape_xl.html">scrape_xl</a></span>(<span class="kw">url</span> <span class="kw">=</span> <span class="no">url</span>)
<span class="no">xl_df</span>
<span class="co">#&gt; # A tibble: 10 x 1</span>
<span class="co">#&gt;    `84.54`</span>
<span class="co">#&gt;      &lt;dbl&gt;</span>
<span class="co">#&gt;  1    65.6</span>
<span class="co">#&gt;  2    65.0</span>
<span class="co">#&gt;  3    52.2</span>
<span class="co">#&gt;  4    77.0</span>
<span class="co">#&gt;  5    46.6</span>
<span class="co">#&gt;  6    81.8</span>
<span class="co">#&gt;  7    68.1</span>
<span class="co">#&gt;  8    72.8</span>
<span class="co">#&gt;  9    72.5</span>
<span class="co">#&gt; 10    50.5</span></pre></body></html></div>
<p>We can also specify the particular sheet that we wish to get the data for. The function is a wrapper for <code>read_excel</code> but does all the formatting for the user.</p>
</div>
<div id="scraping-files-from-an-iplant-webpage" class="section level2">
<h2 class="hasAnchor">
<a href="#scraping-files-from-an-iplant-webpage" class="anchor"></a>Scraping files from an iPlant webpage</h2>
<p>Next we will explore scraping multiple files from iPlant. This may already seem like quite a straightfoward operation, but as Sharepoint is limited to individual downloads, and to flow into other functions like <code>mine_text</code>, it can be much easier to download multiple files of any desired file type directly from R using <code>scrape_file</code>.</p>
<div class="sourceCode" id="cb3"><html><body><pre class="r"><span class="co"># The url we want to download the files from.</span>
<span class="no">url</span> <span class="kw">&lt;-</span> <span class="st">"https://iplant.plantandfood.co.nz/Team/sp/DataSci/Administration/Forms/BoardReports.aspx"</span>

<span class="co"># Use the function scrape_file to download the files from the iPlant page.</span>
<span class="fu"><a href="../reference/scrape_file.html">scrape_file</a></span>(<span class="no">url</span>)</pre></body></html></div>
<p>If the destination to where we want to download the files is not specified or does not exist, we notice that a new temporary folder is created and the files are scraped to that directory. The <code>scrape_file</code> function works in three parts. First, <code>scrape_source</code> makes a response request to the desired webpage and, if authentication is successful, will return the contents of that page. This is then passed to <code>scrape_file</code> which extracts the source url of the file. The file can be viewed in the web browser using this url, so if we pass the url of each file directly to the last part, <code>download_file</code>, we will download the files straight from the source url to the local directory. There is no need to specify the source url of the file, as Microsoft Sharepoint follows the exact same format for files stored on the site. The only parameters that are need are the url to the files, and the directory (if needed) to where they should be downloaded.</p>
<p>When the files we wish to download are reports or other text files, <code>scrape_file</code> flows nicely into the next function <code>mine_text</code>.</p>
</div>
<div id="mining-the-text-from-one-or-multiple-files" class="section level2">
<h2 class="hasAnchor">
<a href="#mining-the-text-from-one-or-multiple-files" class="anchor"></a>Mining the text from one or multiple files</h2>
<p>Extracting the text from a document can be quite a tedious task, especially if the actual desired text is embedded amongst other data. <code>mine_text</code> provides a simple and straight-foward way to extract the text from one or multiple documents of the form <code>.pdf</code>, <code>.docx</code> and <code>.doc</code>. We can also specify the section of the text we wish to mine. This can be in the form of a subtitle, or just any character string. For this example, we will use the <code>scrape_file</code> function from the block before to download a number of Data Science group reports. One can specify a folder of documents or the path to a document itself, e.g. <code>temp_folder/test.docx</code>. If a file type is not specified, it will be inferred from each file. This way, the function is general enough to extract text from a folder containing documents of various formats, and return a dataframe containing the text of each file.</p>
<div class="sourceCode" id="cb4"><html><body><pre class="r"><span class="co"># Extract the text from the documents between the titles specified.</span>
<span class="no">text_df</span> <span class="kw">&lt;-</span> <span class="fu"><a href="../reference/mine_text.html">mine_text</a></span>(<span class="kw">type</span> <span class="kw">=</span> <span class="st">"docx"</span>, <span class="kw">path</span> <span class="kw">=</span> <span class="st">"./temp_folder"</span>, <span class="kw">from</span> <span class="kw">=</span> <span class="st">"Impact Highlight"</span>, <span class="kw">to</span> <span class="kw">=</span> <span class="st">"Outputs"</span>)
<span class="no">text_df</span>
<span class="co">#&gt;                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            text</span>
<span class="co">#&gt; 1                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             New Data Science Group and new Data Applications team PFR’s success is based on good quality science that is produced as efficiently and effectively as possible. Data is the key component to this science – for most projects the rate limiting step is no longer collecting the information, rather the processing and analysis. To recognize the importance that efficient data management, processing, analysis and interpretations play, a new group Data Science has been formed, with Linley Jesson as group leader. This group will serve as a central point to improve the quality and efficiency of data processing and analysis throughout the company. The group consists of two teams: Biometrics (led by Ruth Butler) which will continue to play a key role in ensuring the quality of research through encouraging sound trial design, statistical analysis and interpretation of results; and a new team Data Applications (led by Peter Jaksons) to help develop analysis pipelines for new or “difficult” types of data, including machine learning, text and data mining and genomic prediction. To highlight the role that efficient data processing can help to improve our outputs, Data Application team members Linley Jesson and Peter Jaksons have been working with members of the Field Crops group, also in Sustainable Production (Edmar Teixiera, Ellen Hume, Jo Sharp) IKS’s Eric Burgueño and NCI’s Hymmi Kong to deploy the simulation tools APSIM Classic and APSIM Next Generation in High Performance Computing system in powerPlant. The project uses massive parallel computing to automate the generation of input files, run over 500,000 simulations instantly. This tool can simulate crop yield and other key agricultural system’s variables (e.g. water and nitrogen use) across multiple years (e.g. using historical climate or climate change scenarios) and locations (e.g. spatial simulations ranging from within-paddock precision agriculture applications to landscape scale assessments). The gains in performance means we were able to model climate change impacts on crop yields over large regions such as the Hawkes Bay, which is soon to be expanded for all arable land in New Zealand. Running such simulations on a single-core computer would have taken 408 days, as opposed to 1 day spread across over 1000 CPU cores in a high performance computing environment. To analyse and interpret the large number of APSIM output files user-friendly interactive visualization tools have been developed using R-shiny, including one by summer student Doney Zhang which maps the outcome of precision management applications to spatially variable enterprises. Other examples of data pipelines in which the new Data Applications team has already been instrumental in are genomic selection projects for both apples and kiwifruit. In these cases setting up workflows to combine data from various sources (genomic, phenotypic, pedigree) and being able to handle these large data sets improves both the reliability and quality of the research and at the same time reduces the time cost of these and future genomic selection projects. The data applications team will continue to help scientists to develop and increase efficiency for these sorts of pipelines, and improve the type of automated reporting to clients.</span>
<span class="co">#&gt; 2 “Small data, Big impact” In PFR, the majority of data sets collected are “small’, of a size that is easily stored within Excel. Thus, these small datasets remain the back-bone of research data collected by PFR, even with the rise of “big data”. Poor management of such small data results in a major a drain on resources: manipulation to enable analysis is time consuming and frequently error-prone, even to the point of making data unusable. At best, this is a waste of effort and at worst is a loss of potential impact or publications. Therefore, it is imperative for the quality of PFR’s science that we have processes in place to ensure the quality of “small data”, particularly that stored in Excel. To this end, the Data Management in Practice project (DMiP) was initiated in August 2016, and ran for 18 months. The project was supported by Blue Skies money, and the team comprised four biometricians, plus two scientists from outside the Data Science group. We developed several tools, ‘How-to’ documents and guidelines, which are available on the ‘Data Management’ iPlant site (Figure 1). Figure 1: Left: One of the Excel Templates produced by the DMiP team. Right: a page from the Data Management iPlant site. To introduce and advertise these materials, over the last 10 months we visited every PFR site in New Zealand bar the very smallest. We gave over 50 seminars/workshops, and over 300 people attended (Figure 2) - a marathon effort, but with a gratifying level of attendance, and lots of positive responses. Figure 2: Andrew McLachlan gives a DMiP seminar at Lincoln. 95250173990“They are easy to follow and to adapt to different situations. They provide a great base for keeping things organized and standardised, ideal for sharing data but also to simplify analyses”“We have used the template for all new projects since we completed the workshop.  We can see the benefits of it and after the workshop feel confident using it”“Very well organised, the metadata pages are important and I wouldn't otherwise include them”00“They are easy to follow and to adapt to different situations. They provide a great base for keeping things organized and standardised, ideal for sharing data but also to simplify analyses”“We have used the template for all new projects since we completed the workshop.  We can see the benefits of it and after the workshop feel confident using it”“Very well organised, the metadata pages are important and I wouldn't otherwise include them”Selected Comments on the Templates from the Feedback Survey: “They are easy to follow and to adapt to different situations. They provide a great base for keeping things organized and standardised, ideal for sharing data but also to simplify analyses” “We have used the template for all new projects since we completed the workshop.  We can see the benefits of it and after the workshop feel confident using it” “Very well organised, the metadata pages are important and I wouldn't otherwise include them” “They are easy to follow and to adapt to different situations. They provide a great base for keeping things organized and standardised, ideal for sharing data but also to simplify analyses” “We have used the template for all new projects since we completed the workshop.  We can see the benefits of it and after the workshop feel confident using it” “Very well organised, the metadata pages are important and I wouldn't otherwise include them” The feedback survey plus anecdotal evidence from Data Science group members suggests that the project has led to widespread improvements in the management of small data sets thus also improving the useability of data and efficiency of data management. However, the feedback also indicated that substantial follow up work – modifications to materials, more workshops, continuing reinforcement of ideas- is required. An approach to carry out this work will be devised in the next few months. The DMiP team (Ruth Butler, Linley Jesson, Andrew McLachlan, Duncan Hedderley, Melanie Davidson and Gareth Hill).</span>
<span class="co">#&gt; 3                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   The Data Science group is helping PFR to increase its efficiency in handling, managing and exploring data in many ways. To promote a culture of Data Science in the company the group hosted a Data Science Forum - "Everyone a Data Scientist". Over 60 PFR staff attended the two-day event at Lincoln on September 5th and 6th. The meeting started with a discussion on leadership from Roger Robson-Williams about creating a Data Science culture in PFR. Guest speaker Kim Lung Chan, head of Data Science at the Callaghan Institute discussed opportunities for funding for students, research projects and businesses in Data Science and opportunities for increased collaboration between the two institutions. We then had a series of rapid fire talks with themes ranging from Data Management, Statistics, High Performance Computing and New Technologies. The second day ended with a discussion on "How to Solve Our Data Pain Points". These “pain points” included increased automatic data handling and reporting, solutions for long-term data storage, and better data literacy among staff. Many of the solutions revolved around: building a strong and connected Data Science community that regularly engages in training, increasing the numbers of data literate staff, and the importance of metadata (data about data) and consistent formats and descriptors for understanding and sharing data. These discussions help promote a culture of data science at Plant and Food Research, so that our data is Findable, Accessible, Interoperable and Reusable (FAIR). In the long-term increased uptake of data science practices will enable more efficient data capture, handling and processing which can only boost productivity for PFR.</span>
<span class="co">#&gt; 4                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    The Science Data Facilitation Project is a collaborative project between Data Science and IKS aimed at promoting the best management of research data and ensuring data is Findable, Accessable, Interoperable and Reusable. Data Science member Carmel Woods is working with the Science Data Facilitator Melanie Burns to identify and train one staff member from each science team across the organisation in the role of Data Steward. These Data Stewards work with their teams to identify areas for improvement in data management practices, support the team to make this happen, and keep the momentum going. Currently there are 49 teams (out of 77, 64%) with a Data Steward and 35 (45%) have been trained. The Science Data Facilitation project is also collating the types of data that scientists are working with across the organisation, identifying individuals that want to be involved in discussions around good practice, and determining the best method to have these discussions and agree on good practice. The discussions have commenced with two data types (Genome Assembly and RNA Sequence) and documentation is almost complete. We will then seek feedback on the process and look at ways to improve the process when we select the next data type. This project is well on task to improve the culture of science data management within Plant and Food Research.</span>
<span class="co">#&gt; 5                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           Kate Richards in conjunction with Rolf Turner from Auckland University have been working on a paper to provide a statistical framework for data analysis methods that are used in problems that arise in quarantine research and address the problem of estimating lethal dose values in the context of fumigating export logs against several species of pine beetle. The approach is to make use of generalized linear mixed models on the basis of which we obtain interval estimates of a required lethal dose.  They apply a “worst case scenario” paradigm in making a final choice of model.   They first fit a wide range of models (determined by various choices of model characteristics). Then, after eliminating implausible values, find the maximum of the upper endpoints of the confidence intervals that were found. Then they take this maximum to be the (conservative) estimate of the required lethal dose. This framework was applied to a series of datasets exploring the dose of methyl bromide required to reach probit 9 level mortality. The current regulation for export of logs to china require a 16hr fumigation  at a dose of 80-120g/m3 depending on temperature. Methyl bromide is a ozone depleting fumigant where the gas has to be recaptured. Using the framework described in the paper they were able to predict a lethal dose estimate 50% less than that currently required. This new dose of 40g/m3 is currently be tested in large scale at port trials. Plot of the “fit” of the chosen model. The blue dots are the observed (“raw”) success fractions. The red plus signs are the fitted success fractions with the (random) influence of the replicates included. The black line is the fitted success fraction formed using fixed effects only.</span>
<span class="co">#&gt; 6                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           Members of the Data Applications team have successfully applied computer vision algorithms to two problems: detecting and counting kiwifruit on the vine, and insects on sticky traps. These projects automate activities which are otherwise performed manually. This saves time and money, and allows larger quantities of data to be collected accurately. Interest in computer vision is increasing within PFR and from commercial partners such as Zespri and the seafood industry. Projects such as these help us build capabilities for PFR’s Growing Futures and Technology Development objectives. Fig. 1. Predicted assignment of A) insects on sticky-traps and B) kiwifruit on vines  using machine-learning computer vision algorthms.</span>
<span class="co">#&gt; 7                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  We are investigating natural language processing techniques for extracting relationships from unstructured data. For example, analysing comments from sensory tests on kiwifruit cultivars to reveal underlying linkages between crop genotypes (e.g. Gold3) and their associated sensory terms (e.g. bitter, grainy, sweet, berry-like). This involves parsing the textual data into the form of a triple-store (subject-predicate-object), producing a constrained graph network, and then clustering the nodes in the network. Although this research is preliminary, initial prototypes are promising; an example from the 2019 Stage II Gold Kiwifruit testing is shown. Coupled with interactive visualisation platforms, this research will provide industry partners like Zespri with new and insightful knowledge about their sensory data. Further, it sets the foundations for including additional metadata, such as the chemical composition of a fruit or the genetic hierarchies between pedigrees. The hope is to connect various data types to obtain a deeper understanding of Plant &amp; Food Research’s crops. -636087278238Figure  SEQ Figure \\* ARABIC 1: Example directed acyclic graph of sensory concepts.Figure  SEQ Figure \\* ARABIC 1: Example directed acyclic graph of sensory concepts. Figure  SEQ Figure \\* ARABIC 1: Example directed acyclic graph of sensory concepts. Figure  SEQ Figure \\* ARABIC 1: Example directed acyclic graph of sensory concepts. -64075192900 -635245272046Figure  SEQ Figure \\* ARABIC 2: Example directed acyclic graph of sensory concepts (zoomed).Figure  SEQ Figure \\* ARABIC 2: Example directed acyclic graph of sensory concepts (zoomed).-64075327100 Figure  SEQ Figure \\* ARABIC 2: Example directed acyclic graph of sensory concepts (zoomed). Figure  SEQ Figure \\* ARABIC 2: Example directed acyclic graph of sensory concepts (zoomed).</span>
<span class="co">#&gt; 8                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      Give an example here of progress towards delivery of impact through a SIGNIFICANT science achievement.  This is the major part of the report and should be a paragraph or two in length.</span></pre></body></html></div>
<p>As we can see, the text from each of these documents has been scraped between the specified headers and has been appended to a dataframe. This format allows us to apply many preprocessing methods (e.g. tokenization, stemming, lemmatization) before the natural language processing techniques, like extracting keywords, named entities, sentiment analysis, and summarising by important sentences.</p>
</div>
<div id="summarising-text-into-most-important-sentences" class="section level2">
<h2 class="hasAnchor">
<a href="#summarising-text-into-most-important-sentences" class="anchor"></a>Summarising text into most important sentences</h2>
<p>So, we have a dataframe of text where each row pertains to a different report, and we wish to summarise these reports by the <code>n</code> most important sentences. The function <code>mine_summary</code> will do this for us automatically using the the LexRank method which relies on the concept of sentence salience - the quality of being particularly noticeable or important - to identify the most important sentences in the documents.</p>
<div class="sourceCode" id="cb5"><html><body><pre class="r"><span class="co"># Apply the mine_summary function to each element of text in the dataframe.</span>
<span class="kw">for</span> (<span class="no">i</span> <span class="kw">in</span> <span class="fl">1</span>:<span class="fu"><a href="https://rdrr.io/r/base/nrow.html">nrow</a></span>(<span class="no">text_df</span>)) {
  <span class="kw">if</span> (<span class="fu"><a href="https://rdrr.io/r/base/length.html">length</a></span>(<span class="fu"><a href="https://rdrr.io/r/base/grep.html">gregexpr</a></span>(<span class="st">'[[:alnum:]][.!?]'</span>, <span class="no">text_df</span>$<span class="no">text</span>[<span class="no">i</span>])<span class="kw">[[</span><span class="fl">1</span>]]) <span class="kw">&lt;</span> <span class="fl">3</span>) {
    <span class="no">text_df</span> <span class="kw">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/data.frame.html">data.frame</a></span>(<span class="kw">text</span> <span class="kw">=</span> <span class="no">text_df</span>[-<span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span>(<span class="no">i</span>),], <span class="kw">stringsAsFactors</span> <span class="kw">=</span> <span class="fl">FALSE</span>)
  }
}

<span class="no">summaries</span> <span class="kw">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/lapply.html">lapply</a></span>(<span class="no">text_df</span>$<span class="no">text</span>, <span class="kw">FUN</span> <span class="kw">=</span> <span class="no">mine_summary</span>, <span class="kw">n_sentences</span> <span class="kw">=</span> <span class="fl">3</span>)
<span class="co">#&gt; Parsing text into sentences and tokens...DONE</span>
<span class="co">#&gt; Calculating pairwise sentence similarities...DONE</span>
<span class="co">#&gt; Applying LexRank...DONE</span>
<span class="co">#&gt; Formatting Output...DONE</span>
<span class="co">#&gt; Parsing text into sentences and tokens...DONE</span>
<span class="co">#&gt; Calculating pairwise sentence similarities...DONE</span>
<span class="co">#&gt; Applying LexRank...DONE</span>
<span class="co">#&gt; Formatting Output...DONE</span>
<span class="co">#&gt; Parsing text into sentences and tokens...DONE</span>
<span class="co">#&gt; Calculating pairwise sentence similarities...DONE</span>
<span class="co">#&gt; Applying LexRank...DONE</span>
<span class="co">#&gt; Formatting Output...DONE</span>
<span class="co">#&gt; Parsing text into sentences and tokens...DONE</span>
<span class="co">#&gt; Calculating pairwise sentence similarities...DONE</span>
<span class="co">#&gt; Applying LexRank...DONE</span>
<span class="co">#&gt; Formatting Output...DONE</span>
<span class="co">#&gt; Parsing text into sentences and tokens...DONE</span>
<span class="co">#&gt; Calculating pairwise sentence similarities...DONE</span>
<span class="co">#&gt; Applying LexRank...DONE</span>
<span class="co">#&gt; Formatting Output...DONE</span>
<span class="co">#&gt; Parsing text into sentences and tokens...DONE</span>
<span class="co">#&gt; Calculating pairwise sentence similarities...DONE</span>
<span class="co">#&gt; Applying LexRank...DONE</span>
<span class="co">#&gt; Formatting Output...DONE</span>
<span class="co">#&gt; Parsing text into sentences and tokens...DONE</span>
<span class="co">#&gt; Calculating pairwise sentence similarities...DONE</span>
<span class="co">#&gt; Applying LexRank...DONE</span>
<span class="co">#&gt; Formatting Output...DONE</span>

<span class="co"># Append these summaries back to the original dataframe as a new column.</span>
<span class="no">text_df</span>$<span class="no">summary</span> <span class="kw">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/lapply.html">sapply</a></span>(<span class="no">summaries</span>, <span class="no">paste0</span>)
<span class="no">text_df</span>$<span class="no">summary</span>
<span class="co">#&gt; [1] "New Data Science Group and new Data Applications team PFR’s success is based on good quality science that is produced as efficiently and effectively as possible. To highlight the role that efficient data processing can help to improve our outputs, Data Application team members Linley Jesson and Peter Jaksons have been working with members of the Field Crops group, also in Sustainable Production (Edmar Teixiera, Ellen Hume, Jo Sharp) IKS’s Eric Burgueño and NCI’s Hymmi Kong to deploy the simulation tools APSIM Classic and APSIM Next Generation in High Performance Computing system in powerPlant. Other examples of data pipelines in which the new Data Applications team has already been instrumental in are genomic selection projects for both apples and kiwifruit."</span>
<span class="co">#&gt; [2] "To this end, the Data Management in Practice project (DMiP) was initiated in August 2016, and ran for 18 months. We developed several tools, ‘How-to’ documents and guidelines, which are available on the ‘Data Management’ iPlant site (Figure 1). We can see the benefits of it and after the workshop feel confident using it” “Very well organised, the metadata pages are important and I wouldn't otherwise include them” The feedback survey plus anecdotal evidence from Data Science group members suggests that the project has led to widespread improvements in the management of small data sets thus also improving the useability of data and efficiency of data management."                                                                                                    </span>
<span class="co">#&gt; [3] "The Data Science group is helping PFR to increase its efficiency in handling, managing and exploring data in many ways. Many of the solutions revolved around: building a strong and connected Data Science community that regularly engages in training, increasing the numbers of data literate staff, and the importance of metadata (data about data) and consistent formats and descriptors for understanding and sharing data. In the long-term increased uptake of data science practices will enable more efficient data capture, handling and processing which can only boost productivity for PFR."                                                                                                                                                                                    </span>
<span class="co">#&gt; [4] "Currently there are 49 teams (out of 77, 64%) with a Data Steward and 35 (45%) have been trained. We will then seek feedback on the process and look at ways to improve the process when we select the next data type. This project is well on task to improve the culture of science data management within Plant and Food Research."                                                                                                                                                                                                                                                                                                                                                                                                                                                           </span>
<span class="co">#&gt; [5] "The approach is to make use of generalized linear mixed models on the basis of which we obtain interval estimates of a required lethal dose. Then they take this maximum to be the (conservative) estimate of the required lethal dose. Using the framework described in the paper they were able to predict a lethal dose estimate 50% less than that currently required."                                                                                                                                                                                                                                                                                                                                                                                                                      </span>
<span class="co">#&gt; [6] "Members of the Data Applications team have successfully applied computer vision algorithms to two problems: detecting and counting kiwifruit on the vine, and insects on sticky traps. Interest in computer vision is increasing within PFR and from commercial partners such as Zespri and the seafood industry. Predicted assignment of A) insects on sticky-traps and B) kiwifruit on vines  using machine-learning computer vision algorthms."                                                                                                                                                                                                                                                                                                                                               </span>
<span class="co">#&gt; [7] "-636087278238Figure  SEQ Figure \\* ARABIC 1: Example directed acyclic graph of sensory concepts.Figure  SEQ Figure \\* ARABIC 1: Example directed acyclic graph of sensory concepts. Figure  SEQ Figure \\* ARABIC 1: Example directed acyclic graph of sensory concepts. Figure  SEQ Figure \\* ARABIC 1: Example directed acyclic graph of sensory concepts."</span></pre></body></html></div>
<p>From the output, we note that the function automatically tokenizes the text and calculates the sentence similarity and salience based on the LexRank sentence centrality. The three most important sentences for each document are then output. It should also be noted that the size of the text must be reasonably larger than the number of sentences to summarise into for obvious reasons.</p>
</div>
<div id="extracting-the-most-frequent-keywords-from-the-text" class="section level2">
<h2 class="hasAnchor">
<a href="#extracting-the-most-frequent-keywords-from-the-text" class="anchor"></a>Extracting the most frequent keywords from the text</h2>
<p>A nice and simple way to analyse text is to extract the most frequently occurring words. This can then be visualised in a lattice plot or a wordcloud to display what keywords are most frequent in that text. It can be a useful way to get a good idea about what a text concerns. Continuing on from the text that was extracted previously,</p>
<div class="sourceCode" id="cb6"><html><body><pre class="r"><span class="co"># Extract the n most frequent keywords from a text.</span>
<span class="no">keywords</span> <span class="kw">&lt;-</span> <span class="fu"><a href="../reference/mine_keywords.html">mine_keywords</a></span>(<span class="kw">text</span> <span class="kw">=</span> <span class="no">text_df</span>$<span class="no">text</span>[<span class="fl">1</span>], <span class="kw">barplot</span> <span class="kw">=</span> <span class="no">T</span>, <span class="kw">wordcloud</span> <span class="kw">=</span> <span class="no">T</span>)
<span class="co">#&gt; Downloading udpipe model from https://raw.githubusercontent.com/jwijffels/udpipe.models.ud.2.4/master/inst/udpipe-ud-2.4-190531/english-ewt-ud-2.4-190531.udpipe to C:/Data/PFRTextMiner/vignettes/english-ewt-ud-2.4-190531.udpipe</span>
<span class="co">#&gt; Visit https://github.com/jwijffels/udpipe.models.ud.2.4 for model license details</span>
<span class="no">keywords</span>
<span class="co">#&gt;           keyword ngram freq</span>
<span class="co">#&gt; 1            data     1   16</span>
<span class="co">#&gt; 2     application     1    7</span>
<span class="co">#&gt; 3            team     1    6</span>
<span class="co">#&gt; 4             new     1    5</span>
<span class="co">#&gt; 5           group     1    5</span>
<span class="co">#&gt; 6         quality     1    4</span>
<span class="co">#&gt; 7         project     1    4</span>
<span class="co">#&gt; 8         genomic     1    4</span>
<span class="co">#&gt; 9      simulation     1    4</span>
<span class="co">#&gt; 10            use     1    4</span>
<span class="co">#&gt; 11            key     1    3</span>
<span class="co">#&gt; 12           help     1    3</span>
<span class="co">#&gt; 13        develop     1    3</span>
<span class="co">#&gt; 14           tool     1    3</span>
<span class="co">#&gt; 15    performance     1    3</span>
<span class="co">#&gt; 16        climate     1    3</span>
<span class="co">#&gt; 17          large     1    3</span>
<span class="co">#&gt; 18 interpretation     1    2</span>
<span class="co">#&gt; 19         output     1    2</span>
<span class="co">#&gt; 20           file     1    2</span>
<span class="co">#&gt; 21           crop     1    2</span>
<span class="co">#&gt; 22          other     1    2</span>
<span class="co">#&gt; 23       variable     1    2</span>
<span class="co">#&gt; 24         change     1    2</span>
<span class="co">#&gt; 25      precision     1    2</span>
<span class="co">#&gt; 26           such     1    2</span>
<span class="co">#&gt; 27      selection     1    2</span>
<span class="co">#&gt; 28            set     1    2</span>
<span class="co">#&gt; 29           time     1    2</span>
<span class="co">#&gt; 30       limiting     1    1</span></pre></body></html></div>
<p>We can specify a number of different parameters in <code>mine_keywords</code> including the number of keywords to extract and the number of grams we want. Having the option of either a barplot or a wordcloud is also a bonus.</p>
</div>
<div id="analysing-the-sentiment-of-a-text" class="section level2">
<h2 class="hasAnchor">
<a href="#analysing-the-sentiment-of-a-text" class="anchor"></a>Analysing the sentiment of a text</h2>
<p>Sentiment analysis is a interesting way of gaining insight into the meaning of a text. When we think of ‘sentiment’, we tend to categorise words or phrases as being positive, negative or neutral. This can tell us a lot about a text, especially if the text is a list of sensory comments for a particular product. But what happens when we have words like ‘but’, ‘not’, ‘really’, or ‘hardly’? These words would typically be removed when dealing with common English stopwords, but they can also change the sentiment of a text. For that reason, they are known as valence shifters, where valence means the intrinsic positivity or negativity of a word or sentence. Therefore, it is critical that we include them when analysing the sentiment of a text, especially in subjective comments like sensory data. We can use the <code>sentimentr</code> package from Tyler Rinker to perform sentiment analysis on text incorporating valence shifters.</p>
<div class="sourceCode" id="cb7"><html><body><pre class="r"><span class="co"># Analyse the sentiment of a text.</span>
<span class="no">text</span> <span class="kw">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span>(<span class="st">"I really love working at Plant and Food Research. It is great."</span>,
          <span class="st">"Today I investigated methods in sentiment analysis."</span>,
          <span class="st">"The results were not very pleasing. &lt;U+0001F612&gt;"</span>,
          <span class="st">"Sentiment anaylsis is amazing! Especially for emojis. &lt;U+0001F497&gt;&lt;U+0001F600&gt;"</span>)

<span class="no">sentiment</span> <span class="kw">&lt;-</span> <span class="fu"><a href="../reference/mine_sentiment.html">mine_sentiment</a></span>(<span class="kw">text</span> <span class="kw">=</span> <span class="no">text</span>)
<span class="no">sentiment</span>
<span class="co">#&gt;                                                             text sentence_id</span>
<span class="co">#&gt; 1 I really love working at Plant and Food Research. It is great.           1</span>
<span class="co">#&gt; 2            Today I investigated methods in sentiment analysis.           2</span>
<span class="co">#&gt; 3                            The results were not very pleasing.           3</span>
<span class="co">#&gt; 4          Sentiment anaylsis is amazing! Especially for emojis.           4</span>
<span class="co">#&gt;   word_count        sd ave_sentiment direction</span>
<span class="co">#&gt; 1         12 0.3144208         0.511  positive</span>
<span class="co">#&gt; 2          7 0.0000000         0.000   neutral</span>
<span class="co">#&gt; 3          6 0.0000000        -0.184  negative</span>
<span class="co">#&gt; 4          7 0.1767767         0.136  positive</span>
<span class="no">sentiment_by_sentence</span> <span class="kw">&lt;-</span> <span class="fu"><a href="../reference/mine_sentiment.html">mine_sentiment</a></span>(<span class="kw">text</span> <span class="kw">=</span> <span class="no">text</span>, <span class="kw">by</span> <span class="kw">=</span> <span class="st">"sentence"</span>)
<span class="no">sentiment_by_sentence</span>
<span class="co">#&gt;                                                  text sentence_id word_count sd</span>
<span class="co">#&gt; 1   I really love working at Plant and Food Research.           1          9  0</span>
<span class="co">#&gt; 2                                        It is great.           1          3  0</span>
<span class="co">#&gt; 3 Today I investigated methods in sentiment analysis.           2          7  0</span>
<span class="co">#&gt; 4                 The results were not very pleasing.           3          6  0</span>
<span class="co">#&gt; 5                      Sentiment anaylsis is amazing!           4          4  0</span>
<span class="co">#&gt; 6                              Especially for emojis.           4          3  0</span>
<span class="co">#&gt;   ave_sentiment direction</span>
<span class="co">#&gt; 1         0.733  positive</span>
<span class="co">#&gt; 2         0.289  positive</span>
<span class="co">#&gt; 3         0.000   neutral</span>
<span class="co">#&gt; 4        -0.184  negative</span>
<span class="co">#&gt; 5         0.250  positive</span>
<span class="co">#&gt; 6         0.000   neutral</span></pre></body></html></div>
<p>We can see that the first call of the function <code>mine_sentiment</code> analyses the sentiment of each element in the character vector, so we get three separate sentences to analyse. The second call splits each element further into sentences so we get four comments that are analysed.</p>
<div class="sourceCode" id="cb8"><html><body><pre class="r"><span class="co"># Analyse the sentiment of a text.</span>
<span class="no">text</span> <span class="kw">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span>(<span class="st">"I really love working at Plant and Food Research. It is great."</span>,
          <span class="st">"Today I investigated methods in sentiment analysis."</span>,
          <span class="st">"The results were not very pleasing. &lt;U+0001F612&gt;"</span>,
          <span class="st">"Sentiment anaylsis is amazing! Especially for emojis. &lt;U+0001F497&gt;&lt;U+0001F600&gt;"</span>)

<span class="no">sentiment</span> <span class="kw">&lt;-</span> <span class="fu"><a href="../reference/mine_sentiment.html">mine_sentiment</a></span>(<span class="kw">text</span> <span class="kw">=</span> <span class="no">text</span>)
<span class="fu"><a href="../reference/highlight_sentiment.html">highlight_sentiment</a></span>(<span class="no">sentiment</span>, <span class="kw">file</span> <span class="kw">=</span> <span class="fu"><a href="https://rdrr.io/r/base/file.path.html">file.path</a></span>(<span class="st">"./polarity.html"</span>))
<span class="co">#&gt; Warning in data.table::setDT(df): Some columns are a multi-column type (such</span>
<span class="co">#&gt; as a matrix column): [6]. setDT will retain these columns as-is but subsequent</span>
<span class="co">#&gt; operations like grouping and joining may fail. Please consider as.data.table()</span>
<span class="co">#&gt; instead which will create a new column for each embedded column.</span>
<span class="co">#&gt; Saved in ./polarity.html</span>
<span class="co">#&gt; Opening ./polarity.html ...</span>
<span class="kw pkg">htmltools</span><span class="kw ns">::</span><span class="fu"><a href="https://rdrr.io/pkg/htmltools/man/include.html">includeHTML</a></span>(<span class="st">"./polarity.html"</span>)</pre></body></html></div>
<p>&lt;!–htYPE html&gt;


</p>
<meta charset="utf-8">
<title>Polarity</title>
<style>
mark.positive{
    background-color: lightgreen;
    color: black;
}

mark.negative{
    background-color: pink;
    color: black;
}
h1 { 
    display: block;
    font-size: 1.2em;
    margin-top: 0.0em;
    margin-bottom: 0.0em;
    margin-left: 0;
    margin-right: 0;
    font-weight: bold;
}
.indented {
    margin-left: 5%;
    margin-right: 5%;
}
</style>
<h1>1: <em><span style="color: green">+.511</span></em>
</h1>
<p class="indented"><mark class="positive">I really love working at Plant and Food Research. It is great.</mark></p>
<h1>2: <em><span style="color: #D0D0D0">.000</span></em>
</h1>
<p class="indented">Today I investigated methods in sentiment analysis.</p>
<h1>3: <em><span style="color: red">-.184</span></em>
</h1>
<p class="indented"><mark class="negative">The results were not very pleasing.</mark></p>
<h1>4: <em><span style="color: green">+.136</span></em>
</h1>
<p class="indented"><mark class="positive">Sentiment anaylsis is amazing! Especially for emojis.</mark></p>



</div>
</div>
</div>
</div>
</div>
</body>
</html>
